{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 Train Path Animation Rating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Notbook we develop a model that rates animations of a path based on the embedding of the path and the animation and additional features created. For the training we are using the labelled data we created in 03 Create Data for Path Animation Rating Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarah\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:137: UserWarning: NumPy 1.16.5 or above is required for this version of SciPy (detected version 1.16.1)\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "#import seaborn as sn\n",
    "#import matplotlib.pyplot as plt\n",
    "#import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "#from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from src.preprocessing.sm_label_transformer import *\n",
    "from src.models.ordinal_classifier_scikit import *\n",
    "#import matplotlib.pyplot as plt\n",
    "#from matplotlib import pyplot\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from src.models.ordinal_classifier_fnn import *\n",
    "from src.models.coral_loss import *\n",
    "from src.data.sm_dataloader import *\n",
    "from src.preprocessing.sm_label_transformer import *\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2cfcd98af5dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m73\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m73\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m73\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(73)\n",
    "random.seed(73)\n",
    "np.random.seed(73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an_vec_0</th>\n",
       "      <th>an_vec_1</th>\n",
       "      <th>an_vec_2</th>\n",
       "      <th>an_vec_3</th>\n",
       "      <th>an_vec_4</th>\n",
       "      <th>an_vec_5</th>\n",
       "      <th>an_vec_6</th>\n",
       "      <th>an_vec_7</th>\n",
       "      <th>an_vec_8</th>\n",
       "      <th>an_vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>rel_width</th>\n",
       "      <th>rel_x_position</th>\n",
       "      <th>rel_y_position</th>\n",
       "      <th>rel_x_position_to_animations</th>\n",
       "      <th>rel_y_position_to_animations</th>\n",
       "      <th>nr_paths_svg</th>\n",
       "      <th>rating_0</th>\n",
       "      <th>rating_1</th>\n",
       "      <th>rating_2</th>\n",
       "      <th>rating_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054752</td>\n",
       "      <td>0.033838</td>\n",
       "      <td>0.042120</td>\n",
       "      <td>0.039501</td>\n",
       "      <td>0.051404</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395994</td>\n",
       "      <td>0.501511</td>\n",
       "      <td>0.579289</td>\n",
       "      <td>0.714309</td>\n",
       "      <td>0.706974</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395994</td>\n",
       "      <td>0.501511</td>\n",
       "      <td>0.637940</td>\n",
       "      <td>0.714309</td>\n",
       "      <td>0.778553</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.134364</td>\n",
       "      <td>0.847434</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054752</td>\n",
       "      <td>0.033838</td>\n",
       "      <td>0.042120</td>\n",
       "      <td>0.039501</td>\n",
       "      <td>0.051404</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.763775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.395994</td>\n",
       "      <td>0.501511</td>\n",
       "      <td>0.579289</td>\n",
       "      <td>0.714309</td>\n",
       "      <td>0.706974</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   an_vec_0  an_vec_1  an_vec_2  an_vec_3  an_vec_4  an_vec_5  an_vec_6  \\\n",
       "0         0         0         0         1         0         0 -1.000000   \n",
       "1         0         0         0         0         0         1 -1.000000   \n",
       "2         0         0         0         0         1         0 -1.000000   \n",
       "3         1         0         0         0         0         0  0.134364   \n",
       "4         0         0         1         0         0         0 -1.000000   \n",
       "\n",
       "   an_vec_7  an_vec_8  an_vec_9  ...  rel_width  rel_x_position  \\\n",
       "0 -1.000000      -1.0 -1.000000  ...   0.054752        0.033838   \n",
       "1 -1.000000      -1.0 -1.000000  ...   0.395994        0.501511   \n",
       "2 -1.000000      -1.0 -1.000000  ...   0.395994        0.501511   \n",
       "3  0.847434      -1.0 -1.000000  ...   0.054752        0.033838   \n",
       "4 -1.000000      -1.0  0.763775  ...   0.395994        0.501511   \n",
       "\n",
       "   rel_y_position  rel_x_position_to_animations  rel_y_position_to_animations  \\\n",
       "0        0.042120                      0.039501                      0.051404   \n",
       "1        0.579289                      0.714309                      0.706974   \n",
       "2        0.637940                      0.714309                      0.778553   \n",
       "3        0.042120                      0.039501                      0.051404   \n",
       "4        0.579289                      0.714309                      0.706974   \n",
       "\n",
       "   nr_paths_svg  rating_0  rating_1  rating_2  rating_3  \n",
       "0          24.0         1         1         1         0  \n",
       "1          24.0         1         1         1         0  \n",
       "2          24.0         1         1         1         0  \n",
       "3          24.0         0         0         0         0  \n",
       "4          24.0         1         1         0         0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = pd.read_csv(\"data/surrogate_model/sm_train_23042021.csv\")\n",
    "test_dataset = pd.read_csv(\"data/surrogate_model/sm_test_23042021.csv\") \n",
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to decode rating labels as orgininal labels are required here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_dataset.iloc[:,:-4]\n",
    "y_train = train_dataset.iloc[:,-4:]\n",
    "y_train = pd.Series(decode_classes(y_train.to_numpy()).flatten())\n",
    "\n",
    "X_test = test_dataset.iloc[:,:-4]\n",
    "y_test = test_dataset.iloc[:,-4:]\n",
    "y_test = pd.Series(decode_classes(y_test.to_numpy()).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an_vec_0</th>\n",
       "      <th>an_vec_1</th>\n",
       "      <th>an_vec_2</th>\n",
       "      <th>an_vec_3</th>\n",
       "      <th>an_vec_4</th>\n",
       "      <th>an_vec_5</th>\n",
       "      <th>an_vec_6</th>\n",
       "      <th>an_vec_7</th>\n",
       "      <th>an_vec_8</th>\n",
       "      <th>an_vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>diff_fill_r</th>\n",
       "      <th>diff_fill_g</th>\n",
       "      <th>diff_fill_b</th>\n",
       "      <th>rel_height</th>\n",
       "      <th>rel_width</th>\n",
       "      <th>rel_x_position</th>\n",
       "      <th>rel_y_position</th>\n",
       "      <th>rel_x_position_to_animations</th>\n",
       "      <th>rel_y_position_to_animations</th>\n",
       "      <th>nr_paths_svg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.541667</td>\n",
       "      <td>-4.541667</td>\n",
       "      <td>-4.541667</td>\n",
       "      <td>0.084239</td>\n",
       "      <td>0.054752</td>\n",
       "      <td>0.033838</td>\n",
       "      <td>0.042120</td>\n",
       "      <td>0.039501</td>\n",
       "      <td>0.051404</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>0.362888</td>\n",
       "      <td>0.395994</td>\n",
       "      <td>0.501511</td>\n",
       "      <td>0.579289</td>\n",
       "      <td>0.714309</td>\n",
       "      <td>0.706974</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>0.362904</td>\n",
       "      <td>0.395994</td>\n",
       "      <td>0.501511</td>\n",
       "      <td>0.637940</td>\n",
       "      <td>0.714309</td>\n",
       "      <td>0.778553</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.134364</td>\n",
       "      <td>0.847434</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.541667</td>\n",
       "      <td>-4.541667</td>\n",
       "      <td>-4.541667</td>\n",
       "      <td>0.084239</td>\n",
       "      <td>0.054752</td>\n",
       "      <td>0.033838</td>\n",
       "      <td>0.042120</td>\n",
       "      <td>0.039501</td>\n",
       "      <td>0.051404</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.763775</td>\n",
       "      <td>...</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>0.362888</td>\n",
       "      <td>0.395994</td>\n",
       "      <td>0.501511</td>\n",
       "      <td>0.579289</td>\n",
       "      <td>0.714309</td>\n",
       "      <td>0.706974</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   an_vec_0  an_vec_1  an_vec_2  an_vec_3  an_vec_4  an_vec_5  an_vec_6  \\\n",
       "0         0         0         0         1         0         0 -1.000000   \n",
       "1         0         0         0         0         0         1 -1.000000   \n",
       "2         0         0         0         0         1         0 -1.000000   \n",
       "3         1         0         0         0         0         0  0.134364   \n",
       "4         0         0         1         0         0         0 -1.000000   \n",
       "\n",
       "   an_vec_7  an_vec_8  an_vec_9  ...  diff_fill_r  diff_fill_g  diff_fill_b  \\\n",
       "0 -1.000000      -1.0 -1.000000  ...    -4.541667    -4.541667    -4.541667   \n",
       "1 -1.000000      -1.0 -1.000000  ...   102.458333   102.458333   102.458333   \n",
       "2 -1.000000      -1.0 -1.000000  ...   102.458333   102.458333   102.458333   \n",
       "3  0.847434      -1.0 -1.000000  ...    -4.541667    -4.541667    -4.541667   \n",
       "4 -1.000000      -1.0  0.763775  ...   102.458333   102.458333   102.458333   \n",
       "\n",
       "   rel_height  rel_width  rel_x_position  rel_y_position  \\\n",
       "0    0.084239   0.054752        0.033838        0.042120   \n",
       "1    0.362888   0.395994        0.501511        0.579289   \n",
       "2    0.362904   0.395994        0.501511        0.637940   \n",
       "3    0.084239   0.054752        0.033838        0.042120   \n",
       "4    0.362888   0.395994        0.501511        0.579289   \n",
       "\n",
       "   rel_x_position_to_animations  rel_y_position_to_animations  nr_paths_svg  \n",
       "0                      0.039501                      0.051404          24.0  \n",
       "1                      0.714309                      0.706974          24.0  \n",
       "2                      0.714309                      0.778553          24.0  \n",
       "3                      0.039501                      0.051404          24.0  \n",
       "4                      0.714309                      0.706974          24.0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3\n",
       "1    3\n",
       "2    3\n",
       "3    0\n",
       "4    2\n",
       "dtype: int32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace by true labeled dataset if available\n",
    "train_dataset = DatasetSM(train=True, path=\"data/surrogate_model\")\n",
    "test_dataset = DatasetSM(train=False, path=\"data/surrogate_model\")\n",
    "train_dataset.X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# only scale numeric variables that are not one-hot encoded\n",
    "scaler.fit(train_dataset.X[:, 12:])\n",
    "train_dataset.scale(scaler)\n",
    "test_dataset.scale(scaler)\n",
    "\n",
    "with open('models/sm_train_standard_scaler.pkl', 'wb') as output:\n",
    "    pickle.dump(scaler, output, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment data (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 2580, 1: 2143, 2: 3752, 3: 1892, 4: 402}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_train, counts_train = np.unique(y_train, return_counts=True)\n",
    "label_counts = dict(zip(unique_train, counts_train))\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices where data label equals 4\n",
    "i_class4 = np.where(y_train == 4)[0]\n",
    "# Calculate upsample size (mean of class sizes 0-3 - class size 4)\n",
    "upsample_size = round(np.mean([label_counts[i] for i in range(4)])) - label_counts[4]\n",
    "# Get upsample indices\n",
    "i_class4_upsampled = np.random.choice(i_class4, size=upsample_size, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create upsampled dataframe\n",
    "y_train = pd.concat([y_train, y_train[i_class4_upsampled]]).reset_index(drop=True)\n",
    "X_train = pd.concat([X_train, X_train.iloc[i_class4_upsampled,:]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       3\n",
       "1       3\n",
       "2       3\n",
       "3       0\n",
       "4       2\n",
       "       ..\n",
       "6234    4\n",
       "6235    4\n",
       "6236    4\n",
       "6237    4\n",
       "6238    4\n",
       "Length: 6239, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier removal (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an_vec_0</th>\n",
       "      <th>an_vec_1</th>\n",
       "      <th>an_vec_2</th>\n",
       "      <th>an_vec_3</th>\n",
       "      <th>an_vec_4</th>\n",
       "      <th>an_vec_5</th>\n",
       "      <th>an_vec_6</th>\n",
       "      <th>an_vec_7</th>\n",
       "      <th>an_vec_8</th>\n",
       "      <th>an_vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>diff_fill_g</th>\n",
       "      <th>diff_fill_b</th>\n",
       "      <th>rel_height</th>\n",
       "      <th>rel_width</th>\n",
       "      <th>rel_x_position</th>\n",
       "      <th>rel_y_position</th>\n",
       "      <th>rel_x_position_to_animations</th>\n",
       "      <th>rel_y_position_to_animations</th>\n",
       "      <th>nr_paths_svg</th>\n",
       "      <th>anomaly</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.541667</td>\n",
       "      <td>-4.541667</td>\n",
       "      <td>0.084239</td>\n",
       "      <td>0.054752</td>\n",
       "      <td>0.033838</td>\n",
       "      <td>0.042120</td>\n",
       "      <td>0.039501</td>\n",
       "      <td>0.051404</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>0.362888</td>\n",
       "      <td>0.395994</td>\n",
       "      <td>0.501511</td>\n",
       "      <td>0.579289</td>\n",
       "      <td>0.714309</td>\n",
       "      <td>0.706974</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>0.362904</td>\n",
       "      <td>0.395994</td>\n",
       "      <td>0.501511</td>\n",
       "      <td>0.637940</td>\n",
       "      <td>0.714309</td>\n",
       "      <td>0.778553</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.134364</td>\n",
       "      <td>0.847434</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.541667</td>\n",
       "      <td>-4.541667</td>\n",
       "      <td>0.084239</td>\n",
       "      <td>0.054752</td>\n",
       "      <td>0.033838</td>\n",
       "      <td>0.042120</td>\n",
       "      <td>0.039501</td>\n",
       "      <td>0.051404</td>\n",
       "      <td>24.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.763775</td>\n",
       "      <td>...</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>102.458333</td>\n",
       "      <td>0.362888</td>\n",
       "      <td>0.395994</td>\n",
       "      <td>0.501511</td>\n",
       "      <td>0.579289</td>\n",
       "      <td>0.714309</td>\n",
       "      <td>0.706974</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   an_vec_0  an_vec_1  an_vec_2  an_vec_3  an_vec_4  an_vec_5  an_vec_6  \\\n",
       "0         0         0         0         1         0         0 -1.000000   \n",
       "1         0         0         0         0         0         1 -1.000000   \n",
       "2         0         0         0         0         1         0 -1.000000   \n",
       "3         1         0         0         0         0         0  0.134364   \n",
       "4         0         0         1         0         0         0 -1.000000   \n",
       "\n",
       "   an_vec_7  an_vec_8  an_vec_9  ...  diff_fill_g  diff_fill_b  rel_height  \\\n",
       "0 -1.000000      -1.0 -1.000000  ...    -4.541667    -4.541667    0.084239   \n",
       "1 -1.000000      -1.0 -1.000000  ...   102.458333   102.458333    0.362888   \n",
       "2 -1.000000      -1.0 -1.000000  ...   102.458333   102.458333    0.362904   \n",
       "3  0.847434      -1.0 -1.000000  ...    -4.541667    -4.541667    0.084239   \n",
       "4 -1.000000      -1.0  0.763775  ...   102.458333   102.458333    0.362888   \n",
       "\n",
       "   rel_width  rel_x_position  rel_y_position  rel_x_position_to_animations  \\\n",
       "0   0.054752        0.033838        0.042120                      0.039501   \n",
       "1   0.395994        0.501511        0.579289                      0.714309   \n",
       "2   0.395994        0.501511        0.637940                      0.714309   \n",
       "3   0.054752        0.033838        0.042120                      0.039501   \n",
       "4   0.395994        0.501511        0.579289                      0.714309   \n",
       "\n",
       "   rel_y_position_to_animations  nr_paths_svg  anomaly  \n",
       "0                      0.051404          24.0        1  \n",
       "1                      0.706974          24.0        1  \n",
       "2                      0.778553          24.0        1  \n",
       "3                      0.051404          24.0       -1  \n",
       "4                      0.706974          24.0        1  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use of Isolation Forest\n",
    "ifo = IsolationForest(random_state=0).fit(X_train)\n",
    "X_train[['anomaly']] = ifo.predict(X_train)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "out_ind = X_train[X_train['anomaly']==-1].index\n",
    "X_train.drop(out_ind, inplace=True, axis=0)\n",
    "y_train.drop(out_ind, inplace=True, axis=0)\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_train.drop('anomaly', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Grid for Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'log2'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [20, 240, 460, 680, 900, 1120, 1340, 1560, 1780, 2000]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=20, stop=2000, num=10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END bootstrap=True, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=20; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=20; total time=   1.9s\n",
      "[CV] END bootstrap=True, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=20; total time=   2.0s\n",
      "[CV] END bootstrap=True, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=20; total time=   1.9s\n",
      "[CV] END bootstrap=True, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=20; total time=   2.6s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "                   estimator=<src.models.ordinal_classifier_scikit.RandomForestOC object at 0x000001CDEAEA86C8>,\n",
       "                   n_iter=1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'log2'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [20, 240, 460, 680, 900,\n",
       "                                                         1120, 1340, 1560, 1780,\n",
       "                                                         2000]},\n",
       "                   random_state=42, scoring='neg_mean_absolute_error',\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# Define stratified cross validation\n",
    "cross_val = StratifiedKFold(n_splits=5)\n",
    "# First create the base model to tune\n",
    "rf = RandomForestOC()\n",
    "# Random search of parameters, using 3 fold cross validation, search across 100 different combinations\n",
    "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=1, cv=cross_val, verbose=2, random_state=42, scoring='neg_mean_absolute_error')\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get best parameters and best evaluation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 20,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'log2',\n",
       " 'max_depth': 50,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.8123291074060035"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train best model on whole training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = rf_random.best_estimator_\n",
    "rf_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate best model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = rf_best.predict(X_test)\n",
    "y_pred_train = rf_best.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label MAE of best random forest classifier on train set: 0.41248026743430216\n",
      "Label MAE of best random forest classifier on test set: 0.787363304981774\n"
     ]
    }
   ],
   "source": [
    "print(f'Label MAE of best random forest classifier on train set: {mean_absolute_error(y_pred_train, y_train)}')\n",
    "print(f'Label MAE of best random forest classifier on test set: {mean_absolute_error(y_pred_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of  best random forest classifier on train set: 0.7060079858854118\n",
      "Accuracy of best random forest classifier on test set: 0.44552450384771164\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy of  best random forest classifier on train set: {accuracy_score(y_pred_train, y_train)}')\n",
    "print(f'Accuracy of best random forest classifier on test set: {accuracy_score(y_pred_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.75      0.61       498\n",
      "           1       0.19      0.13      0.15       414\n",
      "           2       0.47      0.69      0.56       918\n",
      "           3       0.31      0.07      0.11       532\n",
      "           4       0.00      0.00      0.00       107\n",
      "\n",
      "    accuracy                           0.45      2469\n",
      "   macro avg       0.30      0.33      0.29      2469\n",
      "weighted avg       0.38      0.45      0.38      2469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[374  36  82   4   2]\n",
      " [149  54 193  18   0]\n",
      " [120 115 637  46   0]\n",
      " [ 67  64 366  35   0]\n",
      " [ 16  15  65  11   0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/sm_random_forest.sav'\n",
    "pickle.dump(rf_best, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Grid for Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
      " 'max_depth': range(5, 16, 2),\n",
      " 'max_features': range(7, 20, 2),\n",
      " 'min_samples_leaf': range(30, 71, 10),\n",
      " 'min_samples_split': range(200, 1401, 200),\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000],\n",
      " 'subsample': [0.6, 0.7, 0.75, 0.8, 0.85, 0.9]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Boosting learning rate\n",
    "learning_rate = [0.05, 0.10, 0.15, 0.20, 0.25, 0.30]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = range(5,16,2)\n",
    "\n",
    "min_samples_split = range(200,1401,200)\n",
    "\n",
    "min_samples_leaf = range(30,71,10)\n",
    "\n",
    "max_features = range(7,20,2)\n",
    "\n",
    "subsample = [0.6,0.7,0.75,0.8,0.85,0.9]\n",
    "\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'learning_rate': learning_rate,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'max_features': max_features,\n",
    "               'subsample': subsample}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END learning_rate=0.1, max_depth=9, max_features=11, min_samples_leaf=30, min_samples_split=600, n_estimators=1400, subsample=0.75; total time= 1.8min\n",
      "[CV] END learning_rate=0.1, max_depth=9, max_features=11, min_samples_leaf=30, min_samples_split=600, n_estimators=1400, subsample=0.75; total time= 1.9min\n",
      "[CV] END learning_rate=0.1, max_depth=9, max_features=11, min_samples_leaf=30, min_samples_split=600, n_estimators=1400, subsample=0.75; total time= 2.2min\n",
      "[CV] END learning_rate=0.1, max_depth=9, max_features=11, min_samples_leaf=30, min_samples_split=600, n_estimators=1400, subsample=0.75; total time= 2.9min\n",
      "[CV] END learning_rate=0.1, max_depth=9, max_features=11, min_samples_leaf=30, min_samples_split=600, n_estimators=1400, subsample=0.75; total time= 2.8min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "                   estimator=<src.models.ordinal_classifier_scikit.GradientBoostingOC object at 0x000001CDEB1BD888>,\n",
       "                   n_iter=1,\n",
       "                   param_distributions={'learning_rate': [0.05, 0.1, 0.15, 0.2,\n",
       "                                                          0.25, 0.3],\n",
       "                                        'max_depth': range(5, 16, 2),\n",
       "                                        'max_features': range(7, 20, 2),\n",
       "                                        'min_samples_leaf': range(30, 71, 10),\n",
       "                                        'min_samples_split': range(200, 1401, 200),\n",
       "                                        'n_estimators': [200, 400, 600, 800,\n",
       "                                                         1000, 1200, 1400, 1600,\n",
       "                                                         1800, 2000],\n",
       "                                        'subsample': [0.6, 0.7, 0.75, 0.8, 0.85,\n",
       "                                                      0.9]},\n",
       "                   random_state=42, scoring='neg_mean_absolute_error',\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# Define stratified cross validation\n",
    "cross_val = StratifiedKFold(n_splits=5)\n",
    "# First create the base model to tune\n",
    "gb = GradientBoostingOC()\n",
    "# Random search of parameters, using 3 fold cross validation, search across 100 different combinations\n",
    "gb_random = RandomizedSearchCV(estimator=gb, param_distributions=random_grid, n_iter=1, cv=cross_val, verbose=2, random_state=42, scoring = 'neg_mean_absolute_error')\n",
    "# Fit the random search model\n",
    "gb_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get best parameters and best evaluation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.75,\n",
       " 'n_estimators': 1400,\n",
       " 'min_samples_split': 600,\n",
       " 'min_samples_leaf': 30,\n",
       " 'max_features': 11,\n",
       " 'max_depth': 9,\n",
       " 'learning_rate': 0.1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.82913453232539"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_random.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train best model on whole training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_best = gb_random.best_estimator_\n",
    "gb_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate best model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = gb_best.predict(X_test)\n",
    "y_pred_train = gb_best.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label MAE of best gradient boosting classifier on train set: 0.22091187668307177\n",
      "Label MAE of best gradient boosting classifier on test set: 0.7825030376670717\n"
     ]
    }
   ],
   "source": [
    "print(f'Label MAE of best gradient boosting classifier on train set: {mean_absolute_error(y_pred_train, y_train)}')\n",
    "print(f'Label MAE of best gradient boosting classifier on test set: {mean_absolute_error(y_pred_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of  best gradient boosting classifier on train set: 0.8281177453802582\n",
      "Accuracy of best gradient boosting classifier on test set: 0.43458890238963144\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy of  best gradient boosting classifier on train set: {accuracy_score(y_pred_train, y_train)}')\n",
    "print(f'Accuracy of best gradient boosting classifier on test set: {accuracy_score(y_pred_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.62      0.59       498\n",
      "           1       0.22      0.30      0.25       414\n",
      "           2       0.50      0.59      0.54       918\n",
      "           3       0.38      0.18      0.25       532\n",
      "           4       0.00      0.00      0.00       107\n",
      "\n",
      "    accuracy                           0.43      2469\n",
      "   macro avg       0.33      0.34      0.33      2469\n",
      "weighted avg       0.42      0.43      0.42      2469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[308 110  64  15   1]\n",
      " [103 126 149  36   0]\n",
      " [ 82 201 541  91   3]\n",
      " [ 39 117 276  98   2]\n",
      " [  6  23  59  19   0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/sm_gradient_boosting.sav'\n",
    "pickle.dump(gb_best, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Grid for Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'log2'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [20, 240, 460, 680, 900, 1120, 1340, 1560, 1780, 2000]}\n"
     ]
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start=20, stop=2000, num=10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'log2']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END bootstrap=True, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=20; total time=   0.7s\n",
      "[CV] END bootstrap=True, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=20; total time=   0.7s\n",
      "[CV] END bootstrap=True, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=20; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=20; total time=   0.8s\n",
      "[CV] END bootstrap=True, max_depth=50, max_features=log2, min_samples_leaf=2, min_samples_split=10, n_estimators=20; total time=   0.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "                   estimator=<src.models.ordinal_classifier_scikit.ExtraTreesOC object at 0x000001CDEC4A7208>,\n",
       "                   n_iter=1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'log2'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [20, 240, 460, 680, 900,\n",
       "                                                         1120, 1340, 1560, 1780,\n",
       "                                                         2000]},\n",
       "                   random_state=42, scoring='neg_mean_absolute_error',\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# Define stratified cross validation\n",
    "cross_val = StratifiedKFold(n_splits=5)\n",
    "# First create the base model to tune\n",
    "et = ExtraTreesOC()\n",
    "# Random search of parameters, using 3 fold cross validation, search across 100 different combinations\n",
    "et_random = RandomizedSearchCV(estimator = et, param_distributions = random_grid, n_iter = 1, cv = cross_val, verbose=2, random_state=42, scoring = 'neg_mean_absolute_error')\n",
    "# Fit the random search model\n",
    "et_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get best parameters and best evaluation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 20,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'log2',\n",
       " 'max_depth': 50,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7962629071050694"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et_random.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train best model on whole training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_best = et_random.best_estimator_\n",
    "et_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate best model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = et_best.predict(X_test)\n",
    "y_pred_train = et_best.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label MAE of best extra trees classifier on train set: 0.505896554926177\n",
      "Label MAE of best extra trees classifier on test set: 0.7586067233697853\n"
     ]
    }
   ],
   "source": [
    "print(f'Label MAE of best extra trees classifier on train set: {mean_absolute_error(y_pred_train, y_train)}')\n",
    "print(f'Label MAE of best extra trees classifier on test set: {mean_absolute_error(y_pred_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of  best extra trees classifier on train set: 0.6466710000928592\n",
      "Accuracy of best extra trees classifier on test set: 0.46132037262049413\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy of  best extra trees classifier on train set: {accuracy_score(y_pred_train, y_train)}')\n",
    "print(f'Accuracy of best extra trees classifier on test set: {accuracy_score(y_pred_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.75      0.61       498\n",
      "           1       0.23      0.11      0.15       414\n",
      "           2       0.48      0.76      0.59       918\n",
      "           3       0.24      0.04      0.06       532\n",
      "           4       0.00      0.00      0.00       107\n",
      "\n",
      "    accuracy                           0.46      2469\n",
      "   macro avg       0.29      0.33      0.28      2469\n",
      "weighted avg       0.37      0.46      0.38      2469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[374  35  84   5   0]\n",
      " [150  46 206  12   0]\n",
      " [115  63 699  41   0]\n",
      " [ 68  46 398  20   0]\n",
      " [ 17   6  77   7   0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'models/sm_extra_trees.sav'\n",
    "pickle.dump(et_best, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_function = OrdinalClassifierFNN(num_classes=5, layer_sizes=[38,28])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "fitness_function.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fitness_function(test_dataset.X[:5])\n",
    "print(res)\n",
    "torch.sigmoid(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.L1Loss()\n",
    "report_measure = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(fitness_function.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful because it automatically generates batches in the training loop and takes care of shuffling\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "# Stuff to store\n",
    "train_losses = np.zeros(n_epochs)\n",
    "train_bin_maes = np.zeros(n_epochs)\n",
    "train_accuracies = np.zeros(n_epochs)\n",
    "train_label_maes = np.zeros(n_epochs)\n",
    "\n",
    "test_losses = np.zeros(n_epochs)\n",
    "test_bin_maes = np.zeros(n_epochs)\n",
    "test_accuracies = np.zeros(n_epochs)\n",
    "test_label_maes = np.zeros(n_epochs)\n",
    "\n",
    "\n",
    "for it in range(n_epochs):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    train_bin_mae = []\n",
    "    test_bin_mae = []\n",
    "    train_label_mae = []\n",
    "    test_label_mae = []\n",
    "    correct_train = 0\n",
    "    correct_test = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        # move data to GPU\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = fitness_function(inputs)\n",
    "        loss = coral_loss(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss and other report measures\n",
    "        train_loss.append(loss.item())\n",
    "        train_bin_mae.append(report_measure(torch.sigmoid(outputs), targets).item())\n",
    "        train_label_mae.append(mean_absolute_error(decode_classes(targets), decode_classes(torch.sigmoid(outputs))))\n",
    "        \n",
    "        # Count correct predictions\n",
    "        correct_train += (decode_classes(torch.sigmoid(outputs)) == decode_classes(targets)).sum()\n",
    "        \n",
    "    # Get train loss and test loss\n",
    "    train_loss = np.mean(train_loss)\n",
    "    train_bin_mae = np.mean(train_bin_mae)\n",
    "    train_label_mae = np.mean(train_label_mae)\n",
    "    \n",
    "    # Get train accuracy\n",
    "    train_accuracy = correct_train / len(train_dataset.X)\n",
    "    \n",
    "    predicted_classes = []\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = fitness_function(inputs)\n",
    "        loss = coral_loss(outputs, targets)\n",
    "        test_loss.append(loss.item())\n",
    "        test_bin_mae.append(report_measure(torch.sigmoid(outputs), targets).item())\n",
    "        test_label_mae.append(mean_absolute_error(decode_classes(targets), decode_classes(torch.sigmoid(outputs))))\n",
    "        correct_test += (decode_classes(torch.sigmoid(outputs)) == decode_classes(targets)).sum()\n",
    "    test_loss = np.mean(test_loss)\n",
    "    test_bin_mae = np.mean(test_bin_mae)\n",
    "    test_label_mae = np.mean(test_label_mae)\n",
    "    test_accuracy = correct_test / len(test_dataset.X)\n",
    "\n",
    "    # Save losses\n",
    "    train_losses[it] = train_loss\n",
    "    train_bin_maes[it] = train_bin_mae\n",
    "    test_losses[it] = test_loss\n",
    "    test_bin_maes[it] = test_bin_mae\n",
    "    train_label_maes[it] = train_label_mae\n",
    "    test_label_maes[it] = test_label_mae\n",
    "    train_accuracies[it] = train_accuracy\n",
    "    test_accuracies[it] = test_accuracy\n",
    "\n",
    "    if (it + 1) % 10 == 0 or it==0:\n",
    "        print(f'Epoch {it + 1}/{n_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}'\n",
    "              f' Train Binary MAE: {train_bin_mae:.4f}, Test Binary MAE: {test_bin_mae:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Accuracy: {test_accuracy:.4f}, Train Label MAE:  {train_label_mae:.4f}, Test Label MAE:  {test_label_mae:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-026f6c519759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munique_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "unique_train, counts_train = np.unique(decode_classes(train_dataset.y).flatten(), return_counts=True)\n",
    "display(len(train_dataset))\n",
    "dict(zip(unique_train, counts_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_test, counts_test = np.unique(decode_classes(test_dataset.y).flatten(), return_counts=True)\n",
    "display(len(test_dataset.y))\n",
    "dict(zip(unique_test, counts_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy for majority classifier\n",
    "print(f'Test Benchmark Accuracy: {918/2469 :.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label MAE for constant prediction of class 2\n",
    "print(f'Test Benchmark Label MAE: {2*(498+107)/2469 + 1*(414+532)/2469 :.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary MAE for constant prediction of class 2 (1,1,0,0)\n",
    "print(f'Test Benchmark Binary MAE: {(498*0.5 + 414*0.25 + 532*0.25 + 107*0.5)/2469 :.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(test_losses, label='test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_bin_maes, label='train binary MAE')\n",
    "plt.plot(test_bin_maes, label='test binary MAE')\n",
    "plt.hlines(0.2183, xmin=0, xmax=100, color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_label_maes, label='train label MAE')\n",
    "plt.plot(test_label_maes, label='test label MAE')\n",
    "plt.hlines(0.8732, xmin=0, xmax=100, color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_accuracies, label='train accuracy')\n",
    "plt.plot(test_accuracies, label='test accuracy')\n",
    "plt.hlines(0.3718, xmin=0, xmax=100, color='red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(fitness_function.state_dict(), \"models/sm_fnn.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
